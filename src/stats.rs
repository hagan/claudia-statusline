//! Statistics tracking module.
//!
//! This module provides persistent statistics tracking for Claude Code sessions,
//! including costs, line changes, and usage metrics. Statistics are stored in
//! both JSON and SQLite formats for reliability and concurrent access.
//!
//! **Note**: Advanced features (token rates, rolling window) require SQLite.
//! JSON backup mode is deprecated and will be removed in v3.0.

use crate::common::{current_date, current_month, current_timestamp, get_data_dir};
use crate::config::get_config;
use crate::database::SqliteDatabase;
use crate::error::{Result, StatuslineError};
use crate::retry::{retry_if_retryable, RetryConfig};
use fs2::FileExt;
use log::{debug, error, warn};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs::{self, File, OpenOptions};
use std::io::{Read, Seek, Write};
use std::path::{Path, PathBuf};
use std::sync::OnceLock;
use std::time::SystemTime;

/// Static flag to ensure deprecation warning is only shown once per process
static JSON_BACKUP_WARNING_SHOWN: OnceLock<bool> = OnceLock::new();

/// Show deprecation warning for json_backup mode (only once per process)
fn warn_json_backup_deprecated() {
    JSON_BACKUP_WARNING_SHOWN.get_or_init(|| {
        warn!(
            "DEPRECATION: json_backup mode is deprecated and will be removed in v3.0. \
             Advanced features (token rates, rolling window, context learning) require SQLite. \
             Run 'statusline migrate --finalize' to migrate to SQLite-only mode."
        );
        // Also print to stderr for visibility (log might be filtered)
        eprintln!(
            "\x1b[33mâš  DEPRECATION:\x1b[0m json_backup mode is deprecated. \
             Token rates and other advanced features require SQLite. \
             Run 'statusline migrate --finalize' to switch to SQLite-only mode."
        );
        true
    });
}

/// Persistent stats tracking structures
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StatsData {
    pub version: String,
    pub created: String,
    pub last_updated: String,
    pub sessions: HashMap<String, SessionStats>,
    pub daily: HashMap<String, DailyStats>,
    pub monthly: HashMap<String, MonthlyStats>,
    pub all_time: AllTimeStats,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SessionStats {
    pub last_updated: String,
    pub cost: f64,
    pub lines_added: u64,
    pub lines_removed: u64,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub start_time: Option<String>, // ISO 8601 timestamp of session start
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_tokens_observed: Option<u32>, // For adaptive context learning
    #[serde(skip_serializing_if = "Option::is_none")]
    pub active_time_seconds: Option<u64>, // Accumulated active time (for burn_rate.mode = "active_time")
    #[serde(skip_serializing_if = "Option::is_none")]
    pub last_activity: Option<String>, // ISO 8601 timestamp of last activity (for active_time tracking)
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DailyStats {
    pub total_cost: f64,
    pub sessions: Vec<String>,
    pub lines_added: u64,
    pub lines_removed: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MonthlyStats {
    pub total_cost: f64,
    pub sessions: usize,
    pub lines_added: u64,
    pub lines_removed: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct AllTimeStats {
    pub total_cost: f64,
    pub sessions: usize,
    pub since: String,
}

impl Default for StatsData {
    fn default() -> Self {
        let now = current_timestamp();
        StatsData {
            version: "1.0".to_string(),
            created: now.clone(),
            last_updated: now.clone(),
            sessions: HashMap::new(),
            daily: HashMap::new(),
            monthly: HashMap::new(),
            all_time: AllTimeStats {
                total_cost: 0.0,
                sessions: 0,
                since: now,
            },
        }
    }
}

impl StatsData {
    pub fn load() -> Self {
        // Phase 2: Try SQLite first, then fall back to JSON
        if let Ok(data) = Self::load_from_sqlite() {
            return data;
        }

        // Fall back to JSON if SQLite fails
        let path = Self::get_stats_file_path();

        if path.exists() {
            if let Ok(contents) = fs::read_to_string(&path) {
                match serde_json::from_str(&contents) {
                    Ok(data) => {
                        // Migrate JSON data to SQLite if needed
                        if let Err(e) = Self::migrate_to_sqlite(&data) {
                            log::warn!("Failed to migrate JSON to SQLite: {}", e);
                        }
                        return data;
                    }
                    Err(e) => {
                        // File exists but can't be parsed - backup and warn
                        warn!("Failed to parse stats file: {}", e);
                        let backup_path = path.with_extension("backup");
                        let _ = fs::copy(&path, &backup_path);

                        // Fix permissions on backup (fs::copy preserves source permissions)
                        #[cfg(unix)]
                        {
                            use std::os::unix::fs::PermissionsExt;
                            if let Ok(metadata) = fs::metadata(&backup_path) {
                                let mut perms = metadata.permissions();
                                perms.set_mode(0o600);
                                let _ = fs::set_permissions(&backup_path, perms);
                            }
                        }

                        warn!("Backed up corrupted stats to: {:?}", backup_path);
                    }
                }
            }
        }

        // Only create default if file doesn't exist (not if corrupted)
        let default_data = Self::default();
        // Try to save the default, but don't fail if we can't
        let _ = default_data.save();
        default_data
    }

    /// Load stats data from SQLite database (Phase 2)
    pub fn load_from_sqlite() -> Result<Self> {
        let db_path = Self::get_sqlite_path()?;

        // Check if database exists
        if !db_path.exists() {
            return Err(StatuslineError::Database(rusqlite::Error::SqliteFailure(
                rusqlite::ffi::Error::new(rusqlite::ffi::SQLITE_CANTOPEN),
                Some("SQLite database not found".to_string()),
            )));
        }

        let db = SqliteDatabase::new(&db_path)?;

        // Load components
        let sessions = db.get_all_sessions()?;
        let daily = db.get_all_daily_stats()?;
        let monthly = db.get_all_monthly_stats()?;
        let all_time_total = db.get_all_time_total()?;
        let sessions_count = db.get_all_time_sessions_count()?;
        let since_date = db
            .get_earliest_session_date()?
            .unwrap_or_else(current_timestamp);

        // Construct in one go to avoid field reassigns after Default
        let data = StatsData {
            sessions,
            daily,
            monthly,
            all_time: AllTimeStats {
                total_cost: all_time_total,
                sessions: sessions_count,
                since: since_date,
            },
            ..Default::default()
        };

        Ok(data)
    }

    /// Migrate JSON data to SQLite if not already done
    fn migrate_to_sqlite(data: &Self) -> Result<()> {
        let db_path = Self::get_sqlite_path()?;
        let db = SqliteDatabase::new(&db_path)?;

        log::debug!("migrate_to_sqlite: Checking if migration needed");
        log::debug!(
            "migrate_to_sqlite: JSON has {} sessions",
            data.sessions.len()
        );

        // Check if we've already migrated by looking for existing sessions
        let has_sessions = db.has_sessions();
        log::debug!("migrate_to_sqlite: DB has_sessions = {}", has_sessions);

        if !has_sessions {
            log::info!(
                "Migrating {} sessions from JSON to SQLite",
                data.sessions.len()
            );
            // Perform migration
            db.import_sessions(&data.sessions)?;
            log::info!(
                "Successfully migrated {} sessions to SQLite",
                data.sessions.len()
            );
        } else {
            log::debug!("Skipping migration - database already has sessions");
        }

        Ok(())
    }

    pub fn save(&self) -> Result<()> {
        let config = get_config();

        // Save to JSON if backup is enabled
        if config.database.json_backup {
            let path = Self::get_stats_file_path();

            // Acquire and lock the file with retry
            let mut file = acquire_stats_file(&path)?;

            // Save the data using our helper
            save_stats_data(&mut file, self);
        } else {
            log::info!("Skipping JSON backup (json_backup=false, SQLite-only mode)");
        }

        // Always save to SQLite (it's now the primary storage)
        perform_sqlite_dual_write(self);

        Ok(())
    }

    pub fn get_stats_file_path() -> PathBuf {
        get_data_dir().join("stats.json")
    }

    pub fn get_sqlite_path() -> Result<PathBuf> {
        Ok(get_data_dir().join("stats.db"))
    }

    pub fn update_session(
        &mut self,
        session_id: &str,
        update: crate::database::SessionUpdate,
    ) -> (f64, f64) {
        let today = current_date();
        let month = current_month();
        let now = current_timestamp();

        // Update SQLite database directly with all parameters including new migration v5 fields
        // This ensures model_name, workspace_dir, device_id, and token breakdown are persisted immediately
        // SqliteDatabase::new() will create the database if it doesn't exist
        // Note: max_tokens_observed will be updated separately from main.rs/lib.rs
        //
        // IMPORTANT: Auto-reset mode deletes then RECREATES the session in the same transaction,
        // so we can't check existence. Instead, compare start_time to detect if session was reset.
        let mut session_was_reset = false;
        let config = crate::config::get_config();

        if let Ok(db_path) = Self::get_sqlite_path() {
            if let Ok(db) = SqliteDatabase::new(&db_path) {
                if let Err(e) = db.update_session(session_id, update.clone()) {
                    log::warn!("Failed to persist session {} to SQLite: {}", session_id, e);
                } else if config.burn_rate.mode == "auto_reset" {
                    // Only check for reset if we're in auto_reset mode
                    // Check if session was reset by comparing start_time
                    // Auto-reset deletes+recreates session, giving it a new start_time
                    if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                        if let Ok(db_start_time) = conn.query_row(
                            "SELECT start_time FROM sessions WHERE session_id = ?1",
                            rusqlite::params![session_id],
                            |row| row.get::<_, String>(0),
                        ) {
                            // Compare with in-memory start_time
                            if let Some(in_memory_session) = self.sessions.get(session_id) {
                                if let Some(ref in_memory_start) = in_memory_session.start_time {
                                    // If start times differ by more than 1 second, session was reset
                                    // (allow small differences due to timestamp precision)
                                    if let (Some(db_time), Some(mem_time)) = (
                                        crate::utils::parse_iso8601_to_unix(&db_start_time),
                                        crate::utils::parse_iso8601_to_unix(in_memory_start),
                                    ) {
                                        let time_diff = db_time.abs_diff(mem_time);

                                        if time_diff > 1 {
                                            // More than 1 second difference = reset
                                            session_was_reset = true;
                                            log::info!(
                                                "Session {} was auto-reset (start_time changed: {} -> {})",
                                                session_id,
                                                in_memory_start,
                                                db_start_time
                                            );
                                            self.sessions.remove(session_id);
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            } else {
                log::warn!(
                    "Failed to open SQLite database at {:?} for session update",
                    db_path
                );
            }
        } else {
            log::warn!("Failed to get SQLite path for session update");
        }

        // Calculate delta from last known session cost
        // If session was reset, treat as new session (delta = full value)
        let last_cost = if session_was_reset {
            0.0
        } else {
            self.sessions.get(session_id).map(|s| s.cost).unwrap_or(0.0)
        };

        let cost_delta = update.cost - last_cost;

        // Calculate line deltas from previous values
        // If session was reset, treat as new session (delta = full value)
        let last_lines_added = if session_was_reset {
            0
        } else {
            self.sessions
                .get(session_id)
                .map(|s| s.lines_added)
                .unwrap_or(0)
        };
        let last_lines_removed = if session_was_reset {
            0
        } else {
            self.sessions
                .get(session_id)
                .map(|s| s.lines_removed)
                .unwrap_or(0)
        };
        let lines_added_delta = (update.lines_added as i64) - (last_lines_added as i64);
        let lines_removed_delta = (update.lines_removed as i64) - (last_lines_removed as i64);

        // Query active_time_seconds and last_activity from SQLite for JSON backup persistence
        let (active_time_seconds, last_activity) = if let Ok(db_path) = Self::get_sqlite_path() {
            if let Ok(conn) = rusqlite::Connection::open(&db_path) {
                conn.query_row(
                    "SELECT active_time_seconds, last_activity FROM sessions WHERE session_id = ?1",
                    rusqlite::params![session_id],
                    |row| {
                        let active_time: Option<i64> = row.get(0).ok();
                        let last_activity: Option<String> = row.get(1).ok();
                        Ok((active_time.map(|t| t as u64), last_activity))
                    },
                )
                .unwrap_or((None, None))
            } else {
                (None, None)
            }
        } else {
            (None, None)
        };

        // Always update session metadata (even with zero/negative deltas)
        // This ensures cost corrections and metadata refreshes are persisted
        // IMPORTANT: Also populate active_time_seconds and last_activity for JSON backup
        if let Some(session) = self.sessions.get_mut(session_id) {
            session.cost = update.cost;
            session.lines_added = update.lines_added;
            session.lines_removed = update.lines_removed;
            session.last_updated = now.clone();
            session.active_time_seconds = active_time_seconds;
            session.last_activity = last_activity.clone();
        } else {
            self.sessions.insert(
                session_id.to_string(),
                SessionStats {
                    last_updated: now.clone(),
                    cost: update.cost,
                    lines_added: update.lines_added,
                    lines_removed: update.lines_removed,
                    start_time: Some(now.clone()), // Track when session started
                    max_tokens_observed: None,     // Will be updated by adaptive learning
                    active_time_seconds,           // Populated from SQLite
                    last_activity,                 // Populated from SQLite
                },
            );
            self.all_time.sessions += 1;
        }

        // IMPORTANT: Check if this session exists for this month BEFORE modifying daily.sessions
        // We must query SQLite for the authoritative answer, since daily.sessions vectors
        // are not persisted and will be empty after a restart (see database.rs:462)
        let mut session_seen_this_month = false;

        // Try to check SQLite first (authoritative source)
        if let Ok(db_path) = Self::get_sqlite_path() {
            if db_path.exists() {
                if let Ok(db) = SqliteDatabase::new(&db_path) {
                    session_seen_this_month = db
                        .session_active_in_month(session_id, &month)
                        .unwrap_or(false);
                }
            }
        }

        // Fallback: If SQLite check failed, check in-memory daily.sessions (works for non-restarted sessions)
        if !session_seen_this_month {
            for (date_key, daily_stats) in &self.daily {
                if date_key.starts_with(&month)
                    && daily_stats.sessions.contains(&session_id.to_string())
                {
                    session_seen_this_month = true;
                    break;
                }
            }
        }

        // Update daily stats
        let daily = self
            .daily
            .entry(today.clone())
            .or_insert_with(|| DailyStats {
                total_cost: 0.0,
                sessions: Vec::new(),
                lines_added: 0,
                lines_removed: 0,
            });

        let is_new_session = !daily.sessions.contains(&session_id.to_string());
        if is_new_session {
            daily.sessions.push(session_id.to_string());
        }
        daily.total_cost += cost_delta;
        // Use deltas instead of absolute totals to avoid double-counting
        daily.lines_added = (daily.lines_added as i64 + lines_added_delta).max(0) as u64;
        daily.lines_removed = (daily.lines_removed as i64 + lines_removed_delta).max(0) as u64;

        // Update monthly stats
        let monthly = self
            .monthly
            .entry(month.clone())
            .or_insert_with(|| MonthlyStats {
                total_cost: 0.0,
                sessions: 0,
                lines_added: 0,
                lines_removed: 0,
            });

        // Increment monthly session count only if this is a new session for the month
        // Note: When loading from SQLite, daily.sessions vectors are empty (we don't persist them),
        // so we rely on the loaded monthly.sessions value and only increment when we see a truly new session
        // We checked session_seen_this_month BEFORE modifying daily.sessions to avoid false positives
        if !session_seen_this_month && is_new_session {
            monthly.sessions += 1;
        }

        monthly.total_cost += cost_delta;
        // Use deltas instead of absolute totals to avoid double-counting
        monthly.lines_added = (monthly.lines_added as i64 + lines_added_delta).max(0) as u64;
        monthly.lines_removed = (monthly.lines_removed as i64 + lines_removed_delta).max(0) as u64;

        // Update all-time stats
        self.all_time.total_cost += cost_delta;

        // Update last modified
        self.last_updated = now;

        // No need to save here - the caller (update_stats_data) handles saving
        // with proper file locking

        // Return current daily and monthly totals
        let daily_total = self.daily.get(&today).map(|d| d.total_cost).unwrap_or(0.0);
        let monthly_total = self
            .monthly
            .get(&month)
            .map(|m| m.total_cost)
            .unwrap_or(0.0);

        (daily_total, monthly_total)
    }

    /// Update max_tokens_observed for a session (adaptive learning)
    /// This should be called after update_session when context usage is calculated
    pub fn update_max_tokens(&mut self, session_id: &str, current_tokens: u32) {
        // Update in-memory stats
        if let Some(session) = self.sessions.get_mut(session_id) {
            let new_max = session.max_tokens_observed.unwrap_or(0).max(current_tokens);
            session.max_tokens_observed = Some(new_max);
        }

        // Persist to SQLite database using dedicated method
        if let Ok(db_path) = Self::get_sqlite_path() {
            if let Ok(db) = SqliteDatabase::new(&db_path) {
                if let Err(e) = db.update_max_tokens_observed(session_id, current_tokens) {
                    log::warn!(
                        "Failed to update max_tokens_observed for session {} in SQLite: {}",
                        session_id,
                        e
                    );
                }
            } else {
                log::warn!(
                    "Failed to open SQLite database at {:?} for max_tokens update",
                    db_path
                );
            }
        } else {
            log::warn!("Failed to get SQLite path for max_tokens update");
        }
    }
}

/// Loads or retrieves the current statistics data.
///
/// This function is process-safe and loads the stats from disk.
///
/// # Returns
///
/// Returns the current `StatsData`, either loaded from disk or a new default instance.
pub fn get_or_load_stats_data() -> StatsData {
    StatsData::load()
}

fn get_stats_backup_path() -> Result<PathBuf> {
    let timestamp = chrono::Local::now().format("%Y%m%d_%H%M%S");
    Ok(get_data_dir().join(format!("stats_backup_{}.json", timestamp)))
}

// Helper function to acquire and lock the stats file with retry
fn acquire_stats_file(path: &Path) -> Result<File> {
    // Ensure directory exists with secure permissions (0o700 on Unix)
    if let Some(parent) = path.parent() {
        #[cfg(unix)]
        {
            use std::os::unix::fs::DirBuilderExt;
            std::fs::DirBuilder::new()
                .mode(0o700)
                .recursive(true)
                .create(parent)?;
        }

        #[cfg(not(unix))]
        {
            fs::create_dir_all(parent)?;
        }
    }

    // Use retry configuration for file operations
    let retry_config = RetryConfig::for_file_ops();

    // Try to open the file with retry and secure permissions (0o600 on Unix)
    let file = retry_if_retryable(&retry_config, || {
        #[cfg(unix)]
        {
            use std::os::unix::fs::OpenOptionsExt;
            OpenOptions::new()
                .read(true)
                .write(true)
                .create(true)
                .truncate(false)
                .mode(0o600) // Owner read/write only on Unix (for new files)
                .open(path)
                .map_err(StatuslineError::from)
        }

        #[cfg(not(unix))]
        {
            OpenOptions::new()
                .read(true)
                .write(true)
                .create(true)
                .truncate(false)
                .open(path)
                .map_err(StatuslineError::from)
        }
    })?;

    // Fix permissions on existing files (mode flag only applies to new files)
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        if let Ok(metadata) = file.metadata() {
            let mut perms = metadata.permissions();
            perms.set_mode(0o600);
            let _ = fs::set_permissions(path, perms); // Best effort - don't fail if it doesn't work
        }
    }

    // Try to acquire exclusive lock with retry (non-blocking)
    // CRITICAL: Use try_lock_exclusive() instead of lock_exclusive()
    // lock_exclusive() blocks indefinitely, causing hangs when multiple
    // Claude instances run simultaneously. try_lock_exclusive() returns
    // immediately with WouldBlock error if lock is held, allowing retry.
    retry_if_retryable(&retry_config, || {
        match file.try_lock_exclusive() {
            Ok(()) => Ok(()),
            Err(e) if e.kind() == std::io::ErrorKind::WouldBlock => {
                // Transient: another process holds the lock, retry is appropriate
                log::debug!("Stats file lock contention (WouldBlock), will retry");
                Err(StatuslineError::lock(
                    "Stats file temporarily locked by another process",
                ))
            }
            Err(e) => {
                // Hard failure: permissions, I/O error, etc.
                log::warn!("Stats file lock failed unexpectedly: {}", e);
                Err(StatuslineError::lock(format!(
                    "Failed to lock stats file: {}",
                    e
                )))
            }
        }
    })?;

    Ok(file)
}

// Helper function to load stats data from file
fn load_stats_data(file: &mut File, path: &Path) -> StatsData {
    let mut contents = String::new();
    if file.read_to_string(&mut contents).is_ok() && !contents.is_empty() {
        match serde_json::from_str(&contents) {
            Ok(data) => {
                // Migrate JSON data to SQLite if needed
                if let Err(e) = StatsData::migrate_to_sqlite(&data) {
                    log::warn!("Failed to migrate JSON to SQLite: {}", e);
                }
                data
            }
            Err(e) => {
                warn!(
                    "Stats file corrupted: {}. Creating backup and starting fresh.",
                    e
                );
                // Try to create a backup of the corrupted file
                if let Ok(backup_path) = get_stats_backup_path() {
                    if let Err(e) = std::fs::copy(path, &backup_path) {
                        error!("Failed to backup corrupted stats file: {}", e);
                    } else {
                        // Fix permissions on backup (fs::copy preserves source permissions)
                        #[cfg(unix)]
                        {
                            use std::os::unix::fs::PermissionsExt;
                            if let Ok(metadata) = fs::metadata(&backup_path) {
                                let mut perms = metadata.permissions();
                                perms.set_mode(0o600);
                                let _ = fs::set_permissions(&backup_path, perms);
                            }
                        }

                        warn!("Corrupted stats backed up to: {:?}", backup_path);
                    }
                }
                StatsData::default()
            }
        }
    } else {
        StatsData::default()
    }
}

// Helper function to save stats data to file
fn save_stats_data(file: &mut File, stats_data: &StatsData) {
    // Write back to file (truncate and write)
    if let Err(e) = file.set_len(0) {
        error!("Failed to truncate stats file: {}", e);
    }
    if let Err(e) = file.seek(std::io::SeekFrom::Start(0)) {
        error!("Failed to seek stats file: {}", e);
    }

    let json = serde_json::to_string_pretty(stats_data).unwrap_or_else(|_| "{}".to_string());
    if let Err(e) = file.write_all(json.as_bytes()) {
        error!("Failed to write stats file: {}", e);
    }
}

// Helper function to write to SQLite (primary storage)
fn perform_sqlite_dual_write(_stats_data: &StatsData) {
    // Write to SQLite (primary storage as of Phase 2)
    let db_path = match StatsData::get_sqlite_path() {
        Ok(p) => p,
        Err(_) => {
            error!("Failed to get SQLite database path");
            return;
        }
    };

    let _db = match SqliteDatabase::new(&db_path) {
        Ok(d) => d,
        Err(e) => {
            error!(
                "Failed to initialize SQLite database at {:?}: {}",
                db_path, e
            );
            return;
        }
    };

    // NOTE: Migration is now handled in load_stats_data() when JSON is loaded
    // Current session is written directly in update_session() with all migration v5 fields
    // No need to call write_current_session_to_sqlite() as that would overwrite model_name/workspace_dir/tokens with NULL
}

/// Updates the statistics data with process-safe file locking.
///
/// This function acquires an exclusive lock on the stats file, loads the current data,
/// applies the update function, and saves the result. It also performs a dual-write
/// to SQLite for better concurrent access.
///
/// # Arguments
///
/// * `updater` - A closure that takes a mutable reference to `StatsData` and returns
///   the daily and monthly totals as a tuple
///
/// # Returns
///
/// Returns a tuple of (daily_total, monthly_total) costs.
///
/// # Example
///
/// ```rust,no_run
/// use statusline::stats::update_stats_data;
/// use statusline::database::SessionUpdate;
///
/// let (daily, monthly) = update_stats_data(|stats| {
///     stats.update_session(
///         "session-123",
///         SessionUpdate {
///             cost: 1.50,
///             lines_added: 100,
///             lines_removed: 50,
///             model_name: None,
///             workspace_dir: None,
///             device_id: None,
///             token_breakdown: None,
///             max_tokens_observed: None,
///             active_time_seconds: None,
///             last_activity: None,
///         },
///     )
/// });
/// ```
pub fn update_stats_data<F>(updater: F) -> (f64, f64)
where
    F: FnOnce(&mut StatsData) -> (f64, f64),
{
    let config = get_config();
    let path = StatsData::get_stats_file_path();

    // Load existing stats data
    let mut stats_data = if config.database.json_backup {
        // Show deprecation warning (once per process)
        warn_json_backup_deprecated();

        // Acquire and lock the file with retry
        let mut file = match acquire_stats_file(&path) {
            Ok(f) => f,
            Err(e) => {
                error!("Failed to acquire stats file after retries: {}", e);
                return (0.0, 0.0);
            }
        };

        let mut data = load_stats_data(&mut file, &path);

        // Apply the update
        let result = updater(&mut data);

        // Save updated stats data to JSON
        save_stats_data(&mut file, &data);

        // Perform SQLite write
        perform_sqlite_dual_write(&data);

        // File lock is automatically released when file is dropped
        return result;
    } else {
        // SQLite-only mode: load from SQLite
        debug!("Operating in SQLite-only mode (json_backup=false)");
        StatsData::load_from_sqlite().unwrap_or_else(|e| {
            warn!("Failed to load from SQLite: {}", e);
            StatsData::default()
        })
    };

    // Apply the update
    let result = updater(&mut stats_data);

    // Save to SQLite (primary storage)
    perform_sqlite_dual_write(&stats_data);

    result
}

/// Get the daily total from stats data
#[allow(dead_code)]
pub fn get_daily_total(data: &StatsData) -> f64 {
    let today = chrono::Local::now().format("%Y-%m-%d").to_string();
    data.daily.get(&today).map(|d| d.total_cost).unwrap_or(0.0)
}

pub fn get_session_duration(session_id: &str) -> Option<u64> {
    let data = get_or_load_stats_data();

    data.sessions.get(session_id).and_then(|session| {
        session.start_time.as_ref().and_then(|start_time| {
            // Parse start time as ISO 8601
            crate::utils::parse_iso8601_to_unix(start_time).and_then(|start_unix| {
                // Get current time
                let now_unix = SystemTime::now()
                    .duration_since(SystemTime::UNIX_EPOCH)
                    .ok()?
                    .as_secs();

                // Return duration in seconds
                Some(now_unix.saturating_sub(start_unix))
            })
        })
    })
}

/// Get session duration in seconds based on configured burn_rate mode
///
/// Respects the `burn_rate.mode` configuration setting:
/// - "wall_clock": Total elapsed time from session start to now (default)
/// - "active_time": Only time spent actively conversing (excludes idle gaps)
/// - "auto_reset": Wall-clock time within current session (resets after inactivity)
pub fn get_session_duration_by_mode(session_id: &str) -> Option<u64> {
    let config = crate::config::get_config();
    let mode = &config.burn_rate.mode;

    match mode.as_str() {
        "active_time" => {
            // Query database for active_time_seconds
            if let Ok(db_path) = StatsData::get_sqlite_path() {
                if db_path.exists() && crate::database::SqliteDatabase::new(&db_path).is_ok() {
                    // Get active_time_seconds from database
                    use rusqlite::Connection;
                    if let Ok(conn) = Connection::open(&db_path) {
                        if let Ok(active_time) = conn.query_row(
                            "SELECT active_time_seconds FROM sessions WHERE session_id = ?1",
                            rusqlite::params![session_id],
                            |row| row.get::<_, Option<i64>>(0),
                        ) {
                            return active_time.map(|t| t as u64);
                        }
                    }
                }
            }
            // Fallback to wall_clock if database query fails
            get_session_duration(session_id)
        }
        "auto_reset" => {
            // Auto-reset mode: Session is archived and recreated after inactivity threshold
            // start_time is reset to current time when recreated, so wall-clock duration
            // represents the current work period (time since last reset)
            get_session_duration(session_id)
        }
        _ => {
            // Wall-clock mode (default): Duration from session start to now (includes idle time)
            get_session_duration(session_id)
        }
    }
}

/// Token rate metrics for display
///
/// All fields are public API for library consumers, even if not all are
/// used internally by the statusline binary.
#[derive(Debug, Clone)]
#[allow(dead_code)] // Public API - fields used by library consumers
pub struct TokenRateMetrics {
    pub input_rate: f64,              // Input tokens per second
    pub output_rate: f64,             // Output tokens per second
    pub cache_read_rate: f64,         // Cache read tokens per second
    pub cache_creation_rate: f64,     // Cache creation tokens per second
    pub total_rate: f64,              // Total tokens per second
    pub duration_seconds: u64,        // Duration used for calculation
    pub cache_hit_ratio: Option<f64>, // Cache hit ratio (0.0-1.0)
    pub cache_roi: Option<f64>,       // Cache ROI (return on investment)
    pub session_total_tokens: u64,    // Total tokens for current session
    pub daily_total_tokens: u64,      // Total tokens for today (across all sessions)
}

/// Calculate token rates for a session
///
/// Uses the same duration mode as burn_rate (wall_clock, active_time, or auto_reset).
/// Returns None if token breakdown or duration is not available.
///
/// When `rate_window_seconds > 0` in config and `transcript_path` is provided,
/// uses rolling window calculation for more responsive rate updates.
/// Otherwise falls back to session average (total_tokens / session_duration).
///
/// Requires a database handle - hot path callers should create the handle once and reuse it.
/// For convenience (non-hot paths), use `calculate_token_rates()` which creates its own handle.
pub fn calculate_token_rates_with_db(
    session_id: &str,
    db: &crate::database::SqliteDatabase,
) -> Option<TokenRateMetrics> {
    calculate_token_rates_with_db_and_transcript(session_id, db, None)
}

/// Calculate token rates with optional rolling window support
///
/// When `rate_window_seconds > 0` in config and `transcript_path` is provided,
/// uses rolling window calculation from transcript for more responsive rate updates.
/// The displayed rate reflects recent activity while totals remain accurate from the database.
pub fn calculate_token_rates_with_db_and_transcript(
    session_id: &str,
    db: &crate::database::SqliteDatabase,
    transcript_path: Option<&str>,
) -> Option<TokenRateMetrics> {
    let config = crate::config::get_config();

    // Check if token rate feature is enabled
    if !config.token_rate.enabled {
        return None;
    }

    // Token rates require SQLite-only mode (json_backup = false)
    // JSON backup doesn't store token breakdowns needed for rate calculation
    if config.database.json_backup {
        log::debug!("Token rates disabled: requires SQLite-only mode (json_backup = false)");
        return None;
    }

    // Get token breakdown from database (for totals - always accurate)
    let (input_tokens, output_tokens, cache_read_tokens, cache_creation_tokens) =
        db.get_session_token_breakdown(session_id)?;

    // Get daily token total
    let daily_total_tokens = db.get_today_token_total().unwrap_or(0);

    // Calculate total tokens (cast to u64 first to prevent overflow with long sessions)
    let total_tokens = input_tokens as u64
        + output_tokens as u64
        + cache_read_tokens as u64
        + cache_creation_tokens as u64;

    // No tokens yet, skip calculation
    if total_tokens == 0 {
        return None;
    }

    // Check if rolling window is configured and transcript is available
    let window_seconds = config.token_rate.rate_window_seconds;
    if window_seconds > 0 {
        if let Some(path) = transcript_path {
            // Try rolling window calculation for OUTPUT rate (responsive)
            // Input rate uses session average (stable, since input_tokens = context size, not delta)
            if let Some((_, rolling_output_rate, _, rolling_cache_creation_rate, window_duration)) =
                crate::utils::get_rolling_window_rates(path, window_seconds)
            {
                // Get session duration for input rate calculation
                let session_duration = if config.token_rate.inherit_duration_mode {
                    get_session_duration_by_mode(session_id).unwrap_or(60)
                } else {
                    get_session_duration(session_id).unwrap_or(60)
                };
                let session_duration_f64 = session_duration.max(1) as f64;

                // Hybrid approach:
                // - Input rate: session average (input_tokens = context size, not cumulative)
                // - Output rate: rolling window (output_tokens ARE cumulative per message)
                // - Cache read rate: session average (like input, represents context)
                // - Cache creation rate: rolling window (like output, cumulative work)
                let input_rate = input_tokens as f64 / session_duration_f64;
                let cache_read_rate = cache_read_tokens as f64 / session_duration_f64;
                let output_rate = rolling_output_rate;
                let cache_creation_rate = rolling_cache_creation_rate;

                let total_rate = input_rate + output_rate + cache_read_rate + cache_creation_rate;

                // Calculate cache metrics from session totals (more stable)
                let (cache_hit_ratio, cache_roi) = calculate_cache_metrics(
                    config,
                    cache_read_tokens,
                    input_tokens,
                    cache_creation_tokens,
                );

                return Some(TokenRateMetrics {
                    input_rate,
                    output_rate,
                    cache_read_rate,
                    cache_creation_rate,
                    total_rate,
                    duration_seconds: window_duration,
                    cache_hit_ratio,
                    cache_roi,
                    session_total_tokens: total_tokens,
                    daily_total_tokens,
                });
            }
            // Fall through to session average if rolling window fails
        }
    }

    // Fall back to session average calculation
    let duration = if config.token_rate.inherit_duration_mode {
        // Use burn_rate.mode
        get_session_duration_by_mode(session_id)?
    } else {
        // Always use wall_clock
        get_session_duration(session_id)?
    };

    // Require at least 60 seconds for meaningful rates
    if duration < 60 {
        return None;
    }

    let duration_f64 = duration as f64;

    // Calculate rates (tokens per second)
    let input_rate = input_tokens as f64 / duration_f64;
    let output_rate = output_tokens as f64 / duration_f64;
    let cache_read_rate = cache_read_tokens as f64 / duration_f64;
    let cache_creation_rate = cache_creation_tokens as f64 / duration_f64;
    let total_rate = total_tokens as f64 / duration_f64;

    // Calculate cache metrics
    let (cache_hit_ratio, cache_roi) = calculate_cache_metrics(
        config,
        cache_read_tokens,
        input_tokens,
        cache_creation_tokens,
    );

    Some(TokenRateMetrics {
        input_rate,
        output_rate,
        cache_read_rate,
        cache_creation_rate,
        total_rate,
        duration_seconds: duration,
        cache_hit_ratio,
        cache_roi,
        session_total_tokens: total_tokens,
        daily_total_tokens,
    })
}

/// Helper to calculate cache metrics (hit ratio and ROI)
fn calculate_cache_metrics(
    config: &crate::config::Config,
    cache_read_tokens: u32,
    input_tokens: u32,
    cache_creation_tokens: u32,
) -> (Option<f64>, Option<f64>) {
    if !config.token_rate.cache_metrics {
        return (None, None);
    }

    // Cache hit ratio: cache_read / (cache_read + input)
    let total_potential_cache = cache_read_tokens + input_tokens;
    let hit_ratio = if total_potential_cache > 0 {
        Some(cache_read_tokens as f64 / total_potential_cache as f64)
    } else {
        None
    };

    // Cache ROI: tokens saved / cost of creating cache
    // ROI = cache_read / cache_creation (how many times we benefited from cache)
    let roi = if cache_creation_tokens > 0 {
        Some(cache_read_tokens as f64 / cache_creation_tokens as f64)
    } else if cache_read_tokens > 0 {
        Some(f64::INFINITY) // Free cache reads (cache created elsewhere)
    } else {
        None
    };

    (hit_ratio, roi)
}

/// Convenience wrapper that creates its own database connection.
///
/// For better performance on hot paths, use `calculate_token_rates_with_db()` with a
/// pre-created database handle.
///
/// Returns None if:
/// - Token rate feature is disabled
/// - Database doesn't exist
/// - Session has no token data
#[allow(dead_code)] // Public API - used by library consumers and tests
pub fn calculate_token_rates(session_id: &str) -> Option<TokenRateMetrics> {
    // Check if token rate feature is enabled before creating db
    let config = crate::config::get_config();
    if !config.token_rate.enabled {
        return None;
    }

    // Create database connection (acceptable overhead for convenience callers)
    let db_path = StatsData::get_sqlite_path().ok()?;
    if !db_path.exists() {
        return None; // Don't create new DB
    }
    let db = crate::database::SqliteDatabase::new(&db_path).ok()?;

    calculate_token_rates_with_db(session_id, &db)
}

/// Test-only: Calculate token rate metrics from raw values without config lookup.
/// This bypasses the OnceLock config to allow deterministic testing.
#[cfg(test)]
pub fn calculate_token_rates_from_raw(
    input_tokens: u32,
    output_tokens: u32,
    cache_read_tokens: u32,
    cache_creation_tokens: u32,
    duration_seconds: u64,
    daily_total_tokens: u64,
) -> Option<TokenRateMetrics> {
    if duration_seconds < 60 {
        return None; // Minimum 60 seconds for stable rates
    }

    let total_tokens = input_tokens as u64
        + output_tokens as u64
        + cache_read_tokens as u64
        + cache_creation_tokens as u64;

    if total_tokens == 0 {
        return None;
    }

    let duration_f64 = duration_seconds as f64;

    let input_rate = input_tokens as f64 / duration_f64;
    let output_rate = output_tokens as f64 / duration_f64;
    let cache_read_rate = cache_read_tokens as f64 / duration_f64;
    let cache_creation_rate = cache_creation_tokens as f64 / duration_f64;
    let total_rate = total_tokens as f64 / duration_f64;

    // Calculate cache metrics
    let total_cache = cache_read_tokens as u64 + cache_creation_tokens as u64;
    let cache_hit_ratio = if total_cache > 0 {
        Some(cache_read_tokens as f64 / total_cache as f64)
    } else {
        None
    };

    let cache_roi = if cache_creation_tokens > 0 {
        // ROI = reads / (creation * cost_multiplier)
        // Cache creation costs 1.25x input, reads cost 0.1x
        // So ROI = (reads * 0.1) / (creation * 1.25) * effective_factor
        // Simplified: reads / (creation * 1.25) shows how many tokens saved per investment
        Some(cache_read_tokens as f64 / (cache_creation_tokens as f64 * 1.25))
    } else if cache_read_tokens > 0 {
        Some(f64::INFINITY) // All reads, no creation cost
    } else {
        None
    };

    Some(TokenRateMetrics {
        input_rate,
        output_rate,
        cache_read_rate,
        cache_creation_rate,
        total_rate,
        duration_seconds,
        cache_hit_ratio,
        cache_roi,
        session_total_tokens: total_tokens,
        daily_total_tokens,
    })
}

#[cfg(test)]
mod tests {
    use super::*;
    use serial_test::serial;
    use std::env;
    use std::path::Path;
    use tempfile::TempDir;

    #[test]
    fn test_stats_data_default() {
        let stats = StatsData::default();
        assert_eq!(stats.version, "1.0");
        assert!(stats.sessions.is_empty());
        assert!(stats.daily.is_empty());
        assert!(stats.monthly.is_empty());
        assert_eq!(stats.all_time.total_cost, 0.0);
        assert_eq!(stats.all_time.sessions, 0);
    }

    #[test]
    fn test_stats_data_update_session() {
        use crate::database::SessionUpdate;
        let mut stats = StatsData::default();
        let (daily, monthly) = stats.update_session(
            "test-session",
            SessionUpdate {
                cost: 10.0,
                lines_added: 100,
                lines_removed: 50,
                model_name: None,
                workspace_dir: None,
                device_id: None,
                token_breakdown: None,
                max_tokens_observed: None,
                active_time_seconds: None,
                last_activity: None,
            },
        );

        assert_eq!(daily, 10.0);
        assert_eq!(monthly, 10.0);
        assert_eq!(stats.all_time.total_cost, 10.0);
        assert_eq!(stats.all_time.sessions, 1);
    }

    #[test]
    #[serial]
    fn test_stats_file_path_xdg() {
        // Set XDG_DATA_HOME for testing
        env::set_var("XDG_DATA_HOME", "/tmp/xdg_test");
        env::set_var("XDG_CONFIG_HOME", "/tmp/xdg_test");
        let path = StatsData::get_stats_file_path();
        assert_eq!(
            path,
            PathBuf::from("/tmp/xdg_test/claudia-statusline/stats.json")
        );
        env::remove_var("XDG_DATA_HOME");
    }

    #[test]
    #[serial]
    fn test_stats_save_and_load() {
        use crate::database::SessionUpdate;
        let temp_dir = TempDir::new().unwrap();
        env::set_var("XDG_DATA_HOME", temp_dir.path().to_str().unwrap());
        env::set_var("XDG_CONFIG_HOME", temp_dir.path().to_str().unwrap());
        env::set_var("STATUSLINE_JSON_BACKUP", "true");

        let mut stats = StatsData::default();
        stats.update_session(
            "test",
            SessionUpdate {
                cost: 5.0,
                lines_added: 50,
                lines_removed: 25,
                model_name: None,
                workspace_dir: None,
                device_id: None,
                token_breakdown: None,
                max_tokens_observed: None,
                active_time_seconds: None,
                last_activity: None,
            },
        );

        let save_result = stats.save();
        assert!(save_result.is_ok());

        // Make sure data was persisted (either JSON or SQLite)
        // Note: In SQLite-only mode, stats.json may not exist
        // Use temp_dir path directly since get_data_dir() uses cached config
        let db_path = temp_dir.path().join("claudia-statusline").join("stats.db");
        assert!(db_path.exists(), "Database should be created");

        // Verify the session was saved to the database by querying directly
        // We can't use StatsData::load() because it uses the cached global config
        use rusqlite::Connection;
        let conn = Connection::open(&db_path).unwrap();
        let session_exists: bool = conn
            .query_row(
                "SELECT COUNT(*) > 0 FROM sessions WHERE session_id = ?1",
                [&"test"],
                |row| row.get(0),
            )
            .unwrap();
        assert!(session_exists, "Session 'test' should exist in database");

        // Verify the cost was saved
        let total_cost: f64 = conn
            .query_row(
                "SELECT SUM(cost) FROM sessions WHERE session_id = ?1",
                [&"test"],
                |row| row.get(0),
            )
            .unwrap();
        assert!(total_cost >= 5.0, "Total cost should be at least 5.0");

        env::remove_var("XDG_DATA_HOME");
    }

    #[test]
    #[serial]
    #[ignore = "Flaky test - OnceLock config caching can cause start_time to differ between runs"]
    fn test_session_start_time_tracking() {
        use crate::database::SessionUpdate;
        use tempfile::TempDir;

        // Isolate from real database
        let temp_dir = TempDir::new().unwrap();
        env::set_var("XDG_DATA_HOME", temp_dir.path());
        env::set_var("XDG_CONFIG_HOME", temp_dir.path());

        let mut stats = StatsData::default();

        // First update creates session with start_time
        stats.update_session(
            "test-session",
            SessionUpdate {
                cost: 1.0,
                lines_added: 10,
                lines_removed: 5,
                model_name: None,
                workspace_dir: None,
                device_id: None,
                token_breakdown: None,
                max_tokens_observed: None,
                active_time_seconds: None,
                last_activity: None,
            },
        );

        // Check that start_time was set
        let session = stats.sessions.get("test-session").unwrap();
        assert!(session.start_time.is_some());

        // Second update to same session shouldn't change start_time
        let original_start = session.start_time.clone();
        stats.update_session(
            "test-session",
            SessionUpdate {
                cost: 2.0,
                lines_added: 20,
                lines_removed: 10,
                model_name: None,
                workspace_dir: None,
                device_id: None,
                token_breakdown: None,
                max_tokens_observed: None,
                active_time_seconds: None,
                last_activity: None,
            },
        );

        let session = stats.sessions.get("test-session").unwrap();
        assert_eq!(session.start_time, original_start);
        assert_eq!(session.cost, 2.0);

        // Cleanup
        env::remove_var("XDG_DATA_HOME");
        env::remove_var("XDG_CONFIG_HOME");
    }

    #[test]
    #[serial]
    #[ignore = "Flaky test - thread synchronization timing issues cause intermittent failures"]
    fn test_concurrent_update_safety() {
        use std::sync::atomic::{AtomicU32, Ordering};
        use std::sync::Arc;
        use std::thread;

        let temp_dir = TempDir::new().unwrap();
        let temp_path = temp_dir.path().to_str().unwrap().to_string();
        env::set_var("XDG_DATA_HOME", &temp_path);
        env::set_var("XDG_CONFIG_HOME", temp_dir.path().to_str().unwrap());

        // Create the directory structure
        let stats_dir = Path::new(&temp_path).join("claudia-statusline");
        std::fs::create_dir_all(&stats_dir).unwrap();

        // Initialize with clean stats file
        let initial_stats = StatsData::default();
        initial_stats.save().unwrap();

        let completed = Arc::new(AtomicU32::new(0));
        let mut handles = vec![];

        // Spawn 10 threads that each add $1.00
        for i in 0..10 {
            let completed_clone = completed.clone();
            let temp_path_clone = temp_path.clone();
            let handle = thread::spawn(move || {
                // Ensure the thread uses the temp directory
                use crate::database::SessionUpdate;
                env::set_var("XDG_DATA_HOME", &temp_path_clone);
                env::set_var("XDG_CONFIG_HOME", &temp_path_clone);
                let (daily, _) = update_stats_data(|stats| {
                    stats.update_session(
                        &format!("test-thread-{}", i),
                        SessionUpdate {
                            cost: 1.0,
                            lines_added: 10,
                            lines_removed: 5,
                            model_name: None,
                            workspace_dir: None,
                            device_id: None,
                            token_breakdown: None,
                            max_tokens_observed: None,
                            active_time_seconds: None,
                            last_activity: None,
                        },
                    )
                });
                completed_clone.fetch_add(1, Ordering::SeqCst);
                daily
            });
            handles.push(handle);
        }

        // Wait for all threads
        for handle in handles {
            handle.join().unwrap();
        }

        // Verify all updates were applied
        assert_eq!(completed.load(Ordering::SeqCst), 10);

        // Load final stats and check total
        let final_stats = StatsData::load();

        // Count the sessions created
        let test_sessions: Vec<_> = final_stats
            .sessions
            .keys()
            .filter(|k| k.starts_with("test-thread-"))
            .collect();

        // Should have created 10 sessions
        assert_eq!(
            test_sessions.len(),
            10,
            "Should have created 10 test sessions"
        );

        // Each session should have $1.00
        for session_id in test_sessions {
            let session = final_stats.sessions.get(session_id).unwrap();
            assert_eq!(session.cost, 1.0, "Each session should have $1.00");
        }

        env::remove_var("XDG_DATA_HOME");
    }

    #[test]
    #[serial]
    fn test_get_session_duration() {
        // Skip this test in CI due to timing issues
        if env::var("CI").is_ok() {
            println!("Skipping test_get_session_duration in CI environment");
            return;
        }
        use std::thread;
        use std::time::Duration;

        let temp_dir = TempDir::new().unwrap();
        let temp_path = temp_dir.path().to_str().unwrap();
        env::set_var("XDG_DATA_HOME", temp_path);
        env::set_var("XDG_CONFIG_HOME", temp_dir.path().to_str().unwrap());

        // Create the directory structure
        let stats_dir = Path::new(&temp_path).join("claudia-statusline");
        std::fs::create_dir_all(&stats_dir).unwrap();

        // Initialize with clean stats file
        let initial_stats = StatsData::default();
        initial_stats.save().unwrap();

        // Create a session with a specific start time
        use crate::database::SessionUpdate;
        update_stats_data(|stats| {
            stats.update_session(
                "duration-test-session",
                SessionUpdate {
                    cost: 1.0,
                    lines_added: 10,
                    lines_removed: 5,
                    model_name: None,
                    workspace_dir: None,
                    device_id: None,
                    token_breakdown: None,
                    max_tokens_observed: None,
                    active_time_seconds: None,
                    last_activity: None,
                },
            )
        });

        // Wait a bit to ensure some time passes
        thread::sleep(Duration::from_millis(100));

        // Get duration - should exist
        let duration = get_session_duration("duration-test-session");
        assert!(
            duration.is_some(),
            "Duration should exist for valid session"
        );

        let duration = duration.unwrap();
        // Duration is u64, so it's always non-negative
        assert!(
            duration < 3600,
            "Duration should be less than 1 hour for a test"
        );

        // Non-existent session should return None
        assert!(get_session_duration("non-existent-session").is_none());

        env::remove_var("XDG_DATA_HOME");
    }

    #[test]
    #[serial]
    #[ignore = "Flaky test - file system timing issues cause intermittent failures"]
    fn test_file_corruption_recovery() {
        let temp_dir = TempDir::new().unwrap();
        env::set_var("XDG_DATA_HOME", temp_dir.path().to_str().unwrap());
        env::set_var("XDG_CONFIG_HOME", temp_dir.path().to_str().unwrap());

        let stats_path = StatsData::get_stats_file_path();

        // Create corrupted file
        fs::create_dir_all(stats_path.parent().unwrap()).unwrap();
        fs::write(&stats_path, "not valid json {").unwrap();

        // Load should handle corruption gracefully
        let stats = StatsData::load();
        assert_eq!(stats.version, "1.0");

        // Check that backup was created
        let backup_path = stats_path.with_extension("backup");
        assert!(backup_path.exists(), "Backup file should exist");

        // Verify backup contains corrupted data
        let backup_contents = fs::read_to_string(&backup_path).unwrap();
        assert_eq!(backup_contents, "not valid json {");

        env::remove_var("XDG_DATA_HOME");
    }

    #[test]
    #[cfg(unix)]
    fn test_stats_file_permissions_on_creation() {
        use std::os::unix::fs::PermissionsExt;
        use tempfile::TempDir;

        // Create a temp directory for the test
        let temp_dir = TempDir::new().unwrap();
        let stats_path = temp_dir
            .path()
            .join("claudia-statusline")
            .join("stats.json");

        // Directly call acquire_stats_file() to test file creation with 0o600 permissions
        // This bypasses save() which uses config caching (OnceLock)
        let _file = acquire_stats_file(&stats_path).unwrap();

        // Verify stats.json has 0o600 permissions
        let metadata = fs::metadata(&stats_path).unwrap();
        let mode = metadata.permissions().mode();

        assert_eq!(
            mode & 0o777,
            0o600,
            "stats.json should have 0o600 permissions, got: {:o}",
            mode & 0o777
        );
    }

    #[test]
    #[cfg(unix)]
    #[serial]
    fn test_stats_file_permissions_fixed_on_save() {
        use std::os::unix::fs::PermissionsExt;
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();
        env::set_var("XDG_DATA_HOME", temp_dir.path());
        env::set_var("STATUSLINE_JSON_BACKUP", "true");

        let stats_path = get_data_dir().join("stats.json");

        // Create stats file with world-readable permissions (0o644)
        let stats = StatsData::default();
        let json = serde_json::to_string_pretty(&stats).unwrap();
        fs::create_dir_all(stats_path.parent().unwrap()).unwrap();
        fs::write(&stats_path, json).unwrap();

        let mut perms = fs::metadata(&stats_path).unwrap().permissions();
        perms.set_mode(0o644); // World-readable
        fs::set_permissions(&stats_path, perms).unwrap();

        // Verify it's world-readable before fix
        let mode_before = fs::metadata(&stats_path).unwrap().permissions().mode();
        assert_eq!(mode_before & 0o777, 0o644, "Setup: file should be 0o644");

        // Directly call acquire_stats_file to fix permissions (bypasses config cache)
        let _ = acquire_stats_file(&stats_path).unwrap();

        // Verify permissions were fixed to 0o600
        let metadata = fs::metadata(&stats_path).unwrap();
        let mode = metadata.permissions().mode();

        assert_eq!(
            mode & 0o777,
            0o600,
            "stats.json should be fixed to 0o600 on save, got: {:o}",
            mode & 0o777
        );

        env::remove_var("STATUSLINE_JSON_BACKUP");
        env::remove_var("XDG_DATA_HOME");
    }

    #[test]
    #[cfg(unix)]
    #[serial]
    fn test_backup_file_permissions() {
        use std::os::unix::fs::PermissionsExt;
        use tempfile::TempDir;

        let temp_dir = TempDir::new().unwrap();
        env::set_var("XDG_DATA_HOME", temp_dir.path());
        env::set_var("STATUSLINE_JSON_BACKUP", "true");

        let stats_path = get_data_dir().join("stats.json");

        // Create corrupted stats file
        fs::create_dir_all(stats_path.parent().unwrap()).unwrap();
        fs::write(&stats_path, "not valid json {").unwrap();

        // Load stats (triggers backup creation)
        let _stats = StatsData::load();

        // Verify backup file has 0o600 permissions
        let backup_path = stats_path.with_extension("backup");
        assert!(backup_path.exists(), "Backup should be created");

        let metadata = fs::metadata(&backup_path).unwrap();
        let mode = metadata.permissions().mode();

        assert_eq!(
            mode & 0o777,
            0o600,
            "Backup file should have 0o600 permissions, got: {:o}",
            mode & 0o777
        );

        env::remove_var("STATUSLINE_JSON_BACKUP");
        env::remove_var("XDG_DATA_HOME");
    }

    /// Unit test for token rate calculation math (no config dependency)
    ///
    /// This test verifies the rate calculation formula directly without relying
    /// on global config state, making it stable regardless of test execution order.
    #[test]
    fn test_token_rate_math_direct() {
        // Test values: 1 hour session with known token counts
        let duration_seconds: u64 = 3600; // 1 hour
        let input_tokens: u32 = 18750; // Expected: 5.2 tok/s
        let output_tokens: u32 = 31250; // Expected: 8.7 tok/s
        let cache_read_tokens: u32 = 150000; // Expected: 41.7 tok/s
        let cache_creation_tokens: u32 = 10000; // Expected: 2.8 tok/s

        // Calculate rates
        let duration_f64 = duration_seconds as f64;
        let input_rate = input_tokens as f64 / duration_f64;
        let output_rate = output_tokens as f64 / duration_f64;
        let cache_read_rate = cache_read_tokens as f64 / duration_f64;
        let cache_creation_rate = cache_creation_tokens as f64 / duration_f64;
        let total_tokens = input_tokens as u64
            + output_tokens as u64
            + cache_read_tokens as u64
            + cache_creation_tokens as u64;
        let total_rate = total_tokens as f64 / duration_f64;

        // Verify rates
        assert!(
            (input_rate - 5.208).abs() < 0.01,
            "Input rate should be ~5.2 tok/s, got {}",
            input_rate
        );
        assert!(
            (output_rate - 8.68).abs() < 0.01,
            "Output rate should be ~8.7 tok/s, got {}",
            output_rate
        );
        assert!(
            (cache_read_rate - 41.67).abs() < 0.01,
            "Cache read rate should be ~41.7 tok/s, got {}",
            cache_read_rate
        );
        assert!(
            (cache_creation_rate - 2.78).abs() < 0.01,
            "Cache creation rate should be ~2.8 tok/s, got {}",
            cache_creation_rate
        );
        assert!(
            (total_rate - 58.33).abs() < 0.1,
            "Total rate should be ~58.3 tok/s, got {}",
            total_rate
        );

        // Test cache hit ratio calculation
        let total_cache = cache_read_tokens as f64 + cache_creation_tokens as f64;
        let cache_hit_ratio = cache_read_tokens as f64 / total_cache;
        assert!(
            (cache_hit_ratio - 0.9375).abs() < 0.01,
            "Cache hit ratio should be ~93.75%, got {}",
            cache_hit_ratio
        );

        // Test cache ROI calculation (reads / creation cost)
        // ROI = cache_read_tokens / (cache_creation_tokens * 1.25)
        // Assuming cache write costs 1.25x input
        let cache_roi = cache_read_tokens as f64 / (cache_creation_tokens as f64 * 1.25);
        assert!(
            (cache_roi - 12.0).abs() < 0.1,
            "Cache ROI should be ~12x, got {}",
            cache_roi
        );
    }

    /// Deterministic test using calculate_token_rates_from_raw (bypasses OnceLock config).
    /// This test exercises the full TokenRateMetrics struct calculation without any
    /// dependency on global config state.
    #[test]
    fn test_calculate_token_rates_from_raw() {
        // Test with typical values: 1 hour session
        let metrics = super::calculate_token_rates_from_raw(
            18750,  // input tokens
            31250,  // output tokens
            150000, // cache read tokens
            10000,  // cache creation tokens
            3600,   // 1 hour duration
            500000, // daily total
        )
        .expect("Should return metrics for valid input");

        // Verify rates
        assert!(
            (metrics.input_rate - 5.208).abs() < 0.01,
            "Input rate mismatch: {}",
            metrics.input_rate
        );
        assert!(
            (metrics.output_rate - 8.68).abs() < 0.01,
            "Output rate mismatch: {}",
            metrics.output_rate
        );
        assert!(
            (metrics.cache_read_rate - 41.67).abs() < 0.01,
            "Cache read rate mismatch: {}",
            metrics.cache_read_rate
        );
        assert!(
            (metrics.cache_creation_rate - 2.78).abs() < 0.01,
            "Cache creation rate mismatch: {}",
            metrics.cache_creation_rate
        );
        assert!(
            (metrics.total_rate - 58.33).abs() < 0.1,
            "Total rate mismatch: {}",
            metrics.total_rate
        );

        // Verify totals
        assert_eq!(metrics.session_total_tokens, 210000);
        assert_eq!(metrics.daily_total_tokens, 500000);
        assert_eq!(metrics.duration_seconds, 3600);

        // Verify cache metrics
        let hit_ratio = metrics
            .cache_hit_ratio
            .expect("Should have cache hit ratio");
        assert!(
            (hit_ratio - 0.9375).abs() < 0.01,
            "Cache hit ratio mismatch: {}",
            hit_ratio
        );

        let roi = metrics.cache_roi.expect("Should have cache ROI");
        assert!((roi - 12.0).abs() < 0.1, "Cache ROI mismatch: {}", roi);
    }

    /// Test that short durations return None (minimum 60 seconds required)
    #[test]
    fn test_calculate_token_rates_from_raw_short_duration() {
        let metrics = super::calculate_token_rates_from_raw(
            1000, 1000, 0, 0, 30, // 30 seconds - too short
            0,
        );
        assert!(
            metrics.is_none(),
            "Should return None for duration < 60 seconds"
        );
    }

    /// Test with zero tokens returns None
    #[test]
    fn test_calculate_token_rates_from_raw_zero_tokens() {
        let metrics = super::calculate_token_rates_from_raw(0, 0, 0, 0, 3600, 0);
        assert!(metrics.is_none(), "Should return None for zero tokens");
    }

    /// Test cache metrics edge cases
    #[test]
    fn test_calculate_token_rates_from_raw_cache_edge_cases() {
        // No cache at all
        let metrics = super::calculate_token_rates_from_raw(1000, 1000, 0, 0, 3600, 0)
            .expect("Should return metrics");
        assert!(metrics.cache_hit_ratio.is_none());
        assert!(metrics.cache_roi.is_none());

        // Cache reads only (infinite ROI)
        let metrics = super::calculate_token_rates_from_raw(1000, 1000, 5000, 0, 3600, 0)
            .expect("Should return metrics");
        assert!(metrics.cache_hit_ratio.unwrap() > 0.99);
        assert!(metrics.cache_roi.unwrap().is_infinite());

        // Cache creation only (0 hit ratio, no ROI)
        let metrics = super::calculate_token_rates_from_raw(1000, 1000, 0, 5000, 3600, 0)
            .expect("Should return metrics");
        assert!(metrics.cache_hit_ratio.unwrap() < 0.01);
        assert!(metrics.cache_roi.unwrap() < 0.01);
    }
}
